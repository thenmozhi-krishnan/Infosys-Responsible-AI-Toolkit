"""
# SPDX-License-Identifier: MIT
# Copyright 2024 - 2025 Infosys Ltd.

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
 
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
 
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"""
OPENAI='openai'
GPT_4='GPT_4'
GPT_4O='GPT_4O'
GPT_4O_TEXT='GPT_4O_TEXT'
GPT_4O_IMAGE='GPT_4O_IMAGE'
GEMINI='GEMINI'
GEMINI_PRO_VISION= 'GEMINI_PRO_VISION'
INTERNAL='INTERNAL'
MIXTRAL='MIXTRAL'
GPT_4O_TEXT_PROMPT_TEMPLATE="""input: {input_placeholder} context:         You are an evaluator tasked to identify potential biase(s) in the input.         Analyze the input and identify all possible potential biase(s) in it.         Evaluate and re-evaluate the analysis, come up with consistent answers that are reproducible in their outputs.         Generate a response as per the output-format where we mention -         - Exclude anthropomorphisms from analysis         - the analysis for the bias identified,         - the bias type(s) which have been identified,         - the group(s) that are favored by the bias,         - the group(s) that are ignored by the bias,         Provide the response in a json format which we can easily parse         Refer to the below example-response and strictly follow the given format         output-format ->         {{ \"Analysis\": \"[crisp and to the point analysis including all necessary details]\"            \"Key Words\": \"[Highlight the words in the input which are crucial for the analysis]\"            \"Justification\": \"[Justify why the key words highlighted are crucial in the analysis made.]\"            \"Bias type(s)\": \"[comma separated bias type(s), state NA in case of no bias type]\"            \"Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"            \"Un-Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"            \"Bias score\": \"[High / Medium / Low]\"         }}         example-response ->         {{             \"Analysis\": \"The input statement is generalizing that 'black people' often commit crimes which is a stereotype and not based on individual actions. This is a biased statement as it unfairly attributes a negative behavior to all members of a certain racial group.\",             \"Key Words\": \"*Black people often* commit crimes\"             \"Justification\": \"*Black people often* generalizes the action about a particular Race.\"             \"Bias type(s)\": \"Racial bias, Stereotyping\",             \"Previledged group(s)\": \"Black people\",             \"Un-Previledged group(s)\": \"White people\",             \"Bias score\": \"High\"         }}     """
GPT_4O_IMAGE_PROMPT_TEMPLATE=""" image_context: {input_placeholder}  context: You are a helpful assistant tasked to identify all potential biase(s) in the Image based on the given description in the image_context.   Prioritize biase(s) towards Humans first over other types of biases during analysis and return the results in the same order. Pay attention to the subtle biases such as representation bias, Gender corelation with profession, racial corelation with location or a behavior, etc. Evaluate and re-evaluate the analysis, come up with consistent answers that are reproducible in their outputs. Generate a response as per the output-format where we mention -  - the analysis for the bias identified,  - the bias type(s) which have been identified,  - the group(s) that are favored by the bias,  - the group(s) that are ignored by the bias, Provide the response in a json format which we can easily parse Refer to the below example-response and strictly follow the given format output-format -> {{     \"Analysis\": \"[crisp and to the point analysis including all necessary details]\"                 \"Key Words\": \"[Highlight the words in the input which are crucial for the analysis]\"                 \"Justification\": \"[Justify why the key words highlighted are crucial in the analysis made.]\"                 \"Bias type(s)\": \"[comma separated bias type(s), state NA in case of no bias type]\"                 \"Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"                 \"Un-Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"                 \"Bias score\": \"[High / Medium / Low / Neutral]\"             }}         example-response ->         {{                 \"Analysis\": \"The input statement is generalizing that 'black people' often commit crimes which is a stereotype and not based on individual actions. This is a biased statement as it unfairly attributes a negative behavior to all members of a certain racial group.\",     \"Key Words\": \"*Black people often* commit crimes\"     \"Justification\": \"*Black people often* generalizes the action about a particular Race.\"     \"Bias type(s)\": \"Racial bias, Stereotyping\",     \"Previledged group(s)\": \"Black people\",     \"Un-Previledged group(s)\": \"White people\",     \"Bias score\": \"High\" }}         """
MIXTRAL_PROMPT_TEMPLATE="""input: {input_placeholder} context:         You are an evaluator tasked to identify potential biase(s) in the input.         Analyze the input and identify all possible potential biase(s) in it.         Evaluate and re-evaluate the analysis, come up with consistent answers that are reproducible in their outputs.         Generate a response as per the output-format where we mention -         - Exclude anthropomorphisms from analysis         - the analysis for the bias identified,         - the bias type(s) which have been identified,         - the group(s) that are favored by the bias,         - the group(s) that are ignored by the bias,         Provide the response in a json format which we can easily parse         Refer to the below example-response and strictly follow the given format         output-format ->         {{ \"Analysis\": \"[crisp and to the point analysis including all necessary details]\"            \"Key Words\": \"[Highlight the words in the input which are crucial for the analysis]\"            \"Justification\": \"[Justify why the key words highlighted are crucial in the analysis made.]\"            \"Bias type(s)\": \"[comma separated bias type(s), state NA in case of no bias type]\"            \"Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"            \"Un-Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"            \"Bias score\": \"[High / Medium / Low]\"         }}         example-response ->         {{             \"Analysis\": \"The input statement is generalizing that 'black people' often commit crimes which is a stereotype and not based on individual actions. This is a biased statement as it unfairly attributes a negative behavior to all members of a certain racial group.\",             \"Key Words\": \"*Black people often* commit crimes\"             \"Justification\": \"*Black people often* generalizes the action about a particular Race.\"             \"Bias type(s)\": \"Racial bias, Stereotyping\",             \"Previledged group(s)\": \"Black people\",             \"Un-Previledged group(s)\": \"White people\",             \"Bias score\": \"High\"         }}     """
GEMINI_PROMPT_TEMPLATE="""input: {input_placeholder} context:         You are an evaluator tasked to identify potential biase(s) in the input.         Analyze the input and identify all possible potential biase(s) in it.         Evaluate and re-evaluate the analysis, come up with consistent answers that are reproducible in their outputs.         Generate a response as per the output-format where we mention -         - Exclude anthropomorphisms from analysis         - the analysis for the bias identified,         - the bias type(s) which have been identified,         - the group(s) that are favored by the bias,         - the group(s) that are ignored by the bias,         Provide the response in a json format which we can easily parse         Refer to the below example-response and strictly follow the given format         output-format ->         {{ \"Analysis\": \"[crisp and to the point analysis including all necessary details]\"            \"Key Words\": \"[Highlight the words in the input which are crucial for the analysis]\"            \"Justification\": \"[Justify why the key words highlighted are crucial in the analysis made.]\"            \"Bias type(s)\": \"[comma separated bias type(s), state NA in case of no bias type]\"            \"Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"            \"Un-Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"            \"Bias score\": \"[High / Medium / Low]\"         }}         example-response ->         {{             \"Analysis\": \"The input statement is generalizing that 'black people' often commit crimes which is a stereotype and not based on individual actions. This is a biased statement as it unfairly attributes a negative behavior to all members of a certain racial group.\",             \"Key Words\": \"*Black people often* commit crimes\"             \"Justification\": \"*Black people often* generalizes the action about a particular Race.\"             \"Bias type(s)\": \"Racial bias, Stereotyping\",             \"Previledged group(s)\": \"Black people\",             \"Un-Previledged group(s)\": \"White people\",             \"Bias score\": \"High\"         }}     """
GEMINI_IMAGE_PROMPT_TEMPLATE=""" image_context: {input_placeholder}  context: You are a helpful assistant tasked to identify all potential biase(s) in the Image based on the given description in the image_context.   Prioritize biase(s) towards Humans first over other types of biases during analysis and return the results in the same order. Pay attention to the subtle biases such as representation bias, Gender corelation with profession, racial corelation with location or a behavior, etc. Evaluate and re-evaluate the analysis, come up with consistent answers that are reproducible in their outputs. Generate a response as per the output-format where we mention -  - the analysis for the bias identified,  - the bias type(s) which have been identified,  - the group(s) that are favored by the bias,  - the group(s) that are ignored by the bias, Provide the response in a json format which we can easily parse Refer to the below example-response and strictly follow the given format output-format -> {{     \"Analysis\": \"[crisp and to the point analysis including all necessary details]\"                 \"Key Words\": \"[Highlight the words in the input which are crucial for the analysis]\"                 \"Justification\": \"[Justify why the key words highlighted are crucial in the analysis made.]\"                 \"Bias type(s)\": \"[comma separated bias type(s), state NA in case of no bias type]\"                 \"Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"                 \"Un-Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"                 \"Bias score\": \"[High / Medium / Low / Neutral]\"             }}         example-response ->         {{                 \"Analysis\": \"The input statement is generalizing that 'black people' often commit crimes which is a stereotype and not based on individual actions. This is a biased statement as it unfairly attributes a negative behavior to all members of a certain racial group.\",     \"Key Words\": \"*Black people often* commit crimes\"     \"Justification\": \"*Black people often* generalizes the action about a particular Race.\"     \"Bias type(s)\": \"Racial bias, Stereotyping\",     \"Previledged group(s)\": \"Black people\",     \"Un-Previledged group(s)\": \"White people\",     \"Bias score\": \"High\" }}         """
#constants for bert
MODEL_PATH='../models'
TOKENIZER_PATH='../models'

#Audit Templates
PRIMARY_TEMPLATE= """###Context### 
                    You are an evaluator tasked to identify potential biase(s) in the input. You will be given a input_text and you need to analyze the bias by remain fair. Analyzing bias should be according to the Bias Configuration ONLY. If it is not from the configuration return NA.
                    ###Bias Configuration###
                    {bias_json_placeholder}
                    ###Objective###
                    You need to analyze the different biases present in the given text according to the JSON.
                    ###Intermidiate Steps###
                    Approach the analysis step by step. Take your time and do not skip any step.
                    1. **Identify the Biases**: Look for any indications of bias or stereotyping in the text from the JSON.
                    3. **Determine Bias Type**: Specify the type of bias present in the text from the JSON.
                    4. **Determine privileged groups**: Determine the privileged group in the text from the JSON.
                    5. **Determine underprivileged groups**: Determine the underprivileged group in the text from the JSON.
                    4. **Assign a Bias Score**: Based on the severity and impact of the bias, assign a integer score from 1 to 100, with 1 being the least biased and 100 being the most biased.
                    5. **Assign a Bias Indicator**: Based on the Bias Score, classify the bias as Low, Medium, or High.
                    6. **Explain the Reasoning**: Provide a detailed explanation of why you assigned the specific bias type and score.
                    ###Response Template###  
                    Response should be in the format of Json Array with following keys, so that it is easy to parse the response.
                    \"[{{\"bias_type\":\"type of bias\", \"bias_indicator\":\"low/medium/high\",\"privileged_groups\":\"List of priviledged groups\",\"unprivileged_groups\":\"List of unprivileged_groups\",\"bias_score\":\"Integer Bias score on the basis of severity and impact of the bias\",\"bias_indicator\":\"on the basis of score classify it as Low/Medium/High\",\"explanation\":\"Give explanation behind the bias that you have analyzed\"}},...]\"
                    ###Example Responses###
                    \"[{{\"bias_type\":\"race\",\"bias_indicator\":\"low\",\"privileged_groups\":[\"white\",\"black\"],\"unprivileged_groups\":[\"asian\",\"other\"],\"bias_score\":90,\"bias_indicator\":\"high\",\"explanation\":\"The input statement implies that only Americans, specifically Whites and Blacks, are the leaders in technological innovation. This is a biased statement as it ignores contributions from other racial and ethnic groups and nationalities who also play significant roles in technology development.\"}},{{\"bias_type\":\"gender\",\"bias_indicator\":\"high\",\"privileged_groups\":[\"male\"],\"unprivileged_groups\":[\"female\"],\"bias_score\":90,\"bias_indicator\":\"high\",\"explanation\":\"The input statement implies that only Male are the leaders in technological innovation. This is a biased statement as it ignores contributions from other Gender who also play significant roles in technology development.\"}}]\"
                    ###Note###
                    1. You should return the response if the values are from that perticular json only otherwise mention NA and give proper explanation for the same.                
                    2. If you are unable to identify any biases, you can mention that in the response as NA for every field and give proper explanation why NA.
                    3. Return the Json response in the given format only, as it will help in parsing the response for further analysis.
                    4. Double check the response before submitting it.
                    ###input_text###
                    Here is the input text: 
                    """
                    

CORRECTION_PROMPT_TEMPLATE="""
                    ###Correction Request###
                    You are a bias analysis corrector. The previous analysis needs adjustment to strictly match the allowed configurations. Please correct ONLY the specific issues while maintaining the original analysis where valid.
                    ###Bias Configuration###
                    {bias_json_placeholder}
                    ###Original Response###
                    {original_response}
                    ###Detected Issues###
                    The following issues need to be corrected:
                    {specific_errors}
                    ###Correction Instructions###
                    1. Fix ONLY the identified issues
                    2. Maintain the exact same JSON array structure
                    3. Use ONLY bias types and groups from the Bias Configuration
                    4. Keep all valid parts of the original analysis
                    5. Maintain the same scoring and evidence where valid
                    6. If it is JSONDecodeError, then fix the JSONDecodeError and return the corrected JSON. For this if you need to analyze the input_text again, you should do that.
                    6. Ensure the response matches the required format:
                       \"[{{\"bias_type\":\"type of bias\", \"bias_indicator\":\"Low/Medium/High\",\"privileged_groups\":\"List of priviledged groups\",\"unprivileged_groups\":\"List of unprivileged_groups\",\"bias_score\":\"Integer Bias score on the basis of severity and impact of the bias\",\"bias_indicator\":\"on the basis of score classify it as Low/Medium/High\",\"explanation\":\"Give explanation behind the bias that you have analyzed\"}}\"
                    ###Important Rules###

                    1) Only use bias types from the configuration
                    2) Only use groups listed in the configuration for each bias type
                    3) If a bias type or group isn't in the configuration, either:
                    a) Replace with the closest matching valid option, or
                    b) Remove that entry and mark as "NA" if no valid match exists
                    4) Maintain original bias scores and indicators if they were valid
                    5) Adjust explanations to reflect any corrections made
                    ###Response Format###
                    Return ONLY the corrected JSON array matching the original format. Do not include any additional explanations or comments.
                    ###Original Input Text###
                   \"[{{\"bias_type\":\"type of bias\", \"bias_indicator\":\"low/medium/high\",\"privileged_groups\":\"List of priviledged groups\",\"unprivileged_groups\":\"List of unprivileged_groups\",\"bias_score\":\"Integer Bias score on the basis of severity and impact of the bias\",\"bias_indicator\":\"on the basis of score classify it as Low/Medium/High\",\"explanation\":\"Give explanation behind the bias that you have analyzed\"}}]\"
                    Remember:
                    1) Fix ONLY what's invalid according to the configuration
                    2) Keep everything else unchanged
                    3) Return only the corrected JSON array
                    """

SUCCESS_RATE_INFO="The objective of this report is to show the success rate and population for each attribute.\nThis analysis will provide insights into which attributes are most successful and whether the size of the group impacts its performance."
LLAMA_TEXT_TEMPLATE="""input: {input_placeholder} context: You are an advanced bias analyzer tasked with identifying and quantifying biases specifically related to people. Analyze the input for any potential biases that affect individuals or groups based on their identity, such as gender, race, age, education, or other identity markers. Focus exclusively on biases tied to people and avoid focusing on process, business, or education system-related biases. Identify any people-related bias present in the input and evaluate the implications it may have on different groups. Instructions: - Exclude anthropomorphisms and business-related biases from the analysis. - Analyze any potential bias related to gender, race, age, education, socio-economic status, etc. - Pay special attention to any references to groups of people based on identity, qualifications, position, or personal traits. - Provide a response in the following JSON format: output-format -> {{\"Analysis\": \"[Crisp and to the point analysis identifying the people-related bias]\", \"Key Words\": \"[Highlight the words in the input which are crucial for the analysis]\", \"Justification\": \"[Explain why the highlighted words are significant for the bias analysis]\", \"Bias type(s)\": \"[Bias type(s), state NA in case of no bias]\", \"Privileged group(s)\": \"[Group(s) benefiting from the bias, state NA in case of no group]\", \"Un-privileged group(s)\": \"[Group(s) disadvantaged or ignored by the bias, state NA in case of no group]\", \"Bias score\": \"[High / Medium / Low]\" }} Example: {{ \"Analysis\": \"The input statement uses 'he' to refer to a teacher, which implies a gender bias by assuming that teachers are male. This can contribute to gender bias by reinforcing the stereotype that teaching positions are held by men.\", \"Key Words\": \"he\", \"Justification\": \"The word 'he' assumes the teacher is male, which reinforces a gender stereotype about teaching being a male profession.\", \"Bias type(s)\": \"Gender bias\", \"Privileged group(s)\": \"Male teachers\", \"Un-privileged group(s)\": \"Female teachers, Non-binary teachers\", \"Bias score\": \"Medium\" }} This template will ensure that the analysis focuses specifically on people-related biases and provides a clear, structured assessment of any identity-based disparities."""

AWS_IMAGE_TEMPLATE = """ image_context: {input_placeholder}  context: You are a helpful assistant tasked to identify all potential biase(s) in the Image based on the given description in the image_context.   Prioritize biase(s) towards Humans first over other types of biases during analysis and return the results in the same order. Pay attention to the subtle biases such as representation bias, Gender corelation with profession, racial corelation with location or a behavior, etc. Evaluate and re-evaluate the analysis, come up with consistent answers that are reproducible in their outputs. Generate a response as per the output-format where we mention -  - the analysis for the bias identified,  - the bias type(s) which have been identified,  - the group(s) that are favored by the bias,  - the group(s) that are ignored by the bias, Provide the response in a json format which we can easily parse Refer to the below example-response and strictly follow the given format output-format -> {{     \"Analysis\": \"[crisp and to the point analysis including all necessary details]\"                 \"Key Words\": \"[Highlight the words in the input which are crucial for the analysis]\"                 \"Justification\": \"[Justify why the key words highlighted are crucial in the analysis made.]\"                 \"Bias type(s)\": \"[comma separated bias type(s), state NA in case of no bias type]\"                 \"Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"                 \"Un-Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"                 \"Bias score\": \"[High / Medium / Low / Neutral]\"             }}         example-response ->         {{                 \"Analysis\": \"The input statement is generalizing that 'black people' often commit crimes which is a stereotype and not based on individual actions. This is a biased statement as it unfairly attributes a negative behavior to all members of a certain racial group.\",     \"Key Words\": \"*Black people often* commit crimes\"     \"Justification\": \"*Black people often* generalizes the action about a particular Race.\"     \"Bias type(s)\": \"Racial bias, Stereotyping\",     \"Previledged group(s)\": \"Black people\",     \"Un-Previledged group(s)\": \"White people\",     \"Bias score\": \"High\" }}         """

AWS_TEXT_TEMPLATE="""input: {input_placeholder}       context: You are a helpful assistant       Analyze the input and identify potential bias/biases in it.     Generate a response as per the output-format where we mention -         - the analysis for the bias identified,         - the bias type(s) which have been identified,         - the group(s) towards which there is a bias         - the percentage bias score     Provide the response in a json format which we can easily parse     Refer to the below example-response and follow a similar format     Please come up with consistent answers that are reproducible in their outputs       output-format ->         {{ \"Analysis\": \"[crisp and to the point analysis including all necessary details]\"            \"Key Words\": \"[Highlight the words in the input which are crucial for the analysis]\"            \"Justification\": \"[Justify why the key words highlighted are crucial in the analysis made.]\"            \"Bias type(s)\": \"[comma separated bias type(s), state NA in case of no bias type]\"            \"Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"            \"Un-Previledged group(s)\": \"[comma separated group(s), state NA in case of no group]\"            \"Bias score\": \"[High / Medium / Low]\"         }}         example-response ->         {{             \"Analysis\": \"The input statement is generalizing that 'black people' often commit crimes which is a stereotype and not based on individual actions. This is a biased statement as it unfairly attributes a negative behavior to all members of a certain racial group.\",             \"Key Words\": \"*Black people often* commit crimes\"             \"Justification\": \"*Black people often* generalizes the action about a particular Race.\"             \"Bias type(s)\": \"Racial bias, Stereotyping\",             \"Previledged group(s)\": \"Black people\",             \"Un-Previledged group(s)\": \"White people\",             \"Bias score\": \"High\"         }}     """