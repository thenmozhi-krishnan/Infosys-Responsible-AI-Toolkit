<!DOCTYPE html>
<html><head></head><body><div class="main-panel wiki-content">
<div class="description">
<div class="header-wrapper" style="max-width: initial">
<!-- breadcrumbs could be implemented and inserted here -->
<div></div>

</div>
<div class="header-wrapper">
<div class="title-wrapper">
<div class="title">
<h1 style="display: flex; align-items: center;">
                                        Safety
                                    </h1>
</div>
<div class="row" style="display: flex;padding: 0; ">

</div>
</div>
</div>
<div class="content-wrapper">
<style>@media (prefers-color-scheme: dark) { }</style>
<div class="toc-macro client-side-toc-macro conf-macro output-block" data-cssliststyle="none" data-hasbody="false" data-headerelements="H1,H2,H3,H4,H5,H6,H7" data-layout="default" data-local-id="ca1f3aaa-1a08-475d-909b-7ee960361f68" data-macro-id="e12543e4-5e08-4352-8256-dea8ab4f4694" data-macro-name="toc">
<style>[data-colorid=gauugy0jy5]{color:#bf2600} html[data-color-mode=dark] [data-colorid=gauugy0jy5]{color:#ff6640}[data-colorid=xu7erfir9f]{color:#bf2600} html[data-color-mode=dark] [data-colorid=xu7erfir9f]{color:#ff6640}[data-colorid=kqhb8o6ro2]{color:#bf2600} html[data-color-mode=dark] [data-colorid=kqhb8o6ro2]{color:#ff6640}[data-colorid=s50qt7skot]{color:#0747a6} html[data-color-mode=dark] [data-colorid=s50qt7skot]{color:#5999f8}[data-colorid=mxbnkxkiqw]{color:#0747a6} html[data-color-mode=dark] [data-colorid=mxbnkxkiqw]{color:#5999f8}[data-colorid=vedwclvhsp]{color:#bf2600} html[data-color-mode=dark] [data-colorid=vedwclvhsp]{color:#ff6640}[data-colorid=woczv200fn]{color:#bf2600} html[data-color-mode=dark] [data-colorid=woczv200fn]{color:#ff6640}[data-colorid=eey1kuef49]{color:#bf2600} html[data-color-mode=dark] [data-colorid=eey1kuef49]{color:#ff6640}[data-colorid=fgbtlxmepz]{color:#bf2600} html[data-color-mode=dark] [data-colorid=fgbtlxmepz]{color:#ff6640}[data-colorid=khflsum4e7]{color:#bf2600} html[data-color-mode=dark] [data-colorid=khflsum4e7]{color:#ff6640}[data-colorid=s35x9512bd]{color:#0747a6} html[data-color-mode=dark] [data-colorid=s35x9512bd]{color:#5999f8}[data-colorid=c6ojjoyzhx]{color:#bf2600} html[data-color-mode=dark] [data-colorid=c6ojjoyzhx]{color:#ff6640}[data-colorid=ea0pqlcxfw]{color:#bf2600} html[data-color-mode=dark] [data-colorid=ea0pqlcxfw]{color:#ff6640}[data-colorid=y83nacmbgx]{color:#bf2600} html[data-color-mode=dark] [data-colorid=y83nacmbgx]{color:#ff6640}[data-colorid=hke566y4t5]{color:#bf2600} html[data-color-mode=dark] [data-colorid=hke566y4t5]{color:#ff6640}[data-colorid=fkcqoxucyg]{color:#bf2600} html[data-color-mode=dark] [data-colorid=fkcqoxucyg]{color:#ff6640}[data-colorid=m18kjpgwyn]{color:#bf2600} html[data-color-mode=dark] [data-colorid=m18kjpgwyn]{color:#ff6640}[data-colorid=my0uzhhu4z]{color:#bf2600} html[data-color-mode=dark] [data-colorid=my0uzhhu4z]{color:#ff6640}[data-colorid=omzyuqzf76]{color:#0747a6} html[data-color-mode=dark] [data-colorid=omzyuqzf76]{color:#5999f8}[data-colorid=twsj7se2is]{color:#bf2600} html[data-color-mode=dark] [data-colorid=twsj7se2is]{color:#ff6640}[data-colorid=hjyc4at717]{color:#bf2600} html[data-color-mode=dark] [data-colorid=hjyc4at717]{color:#ff6640}[data-colorid=dtgpnriefd]{color:#0747a6} html[data-color-mode=dark] [data-colorid=dtgpnriefd]{color:#5999f8}[data-colorid=zfjfnjmq5c]{color:#0747a6} html[data-color-mode=dark] [data-colorid=zfjfnjmq5c]{color:#5999f8}[data-colorid=eqjspvkaft]{color:#bf2600} html[data-color-mode=dark] [data-colorid=eqjspvkaft]{color:#ff6640}[data-colorid=rcjmet9kuv]{color:#bf2600} html[data-color-mode=dark] [data-colorid=rcjmet9kuv]{color:#ff6640}[data-colorid=x6uag2cf20]{color:#bf2600} html[data-color-mode=dark] [data-colorid=x6uag2cf20]{color:#ff6640}[data-colorid=jsairb90ih]{color:#bf2600} html[data-color-mode=dark] [data-colorid=jsairb90ih]{color:#ff6640}[data-colorid=u6g2xw07y8]{color:#bf2600} html[data-color-mode=dark] [data-colorid=u6g2xw07y8]{color:#ff6640}[data-colorid=oiugc8knng]{color:#0747a6} html[data-color-mode=dark] [data-colorid=oiugc8knng]{color:#5999f8}[data-colorid=jd06lgibi7]{color:#0747a6} html[data-color-mode=dark] [data-colorid=jd06lgibi7]{color:#5999f8}[data-colorid=snkcps401k]{color:#0747a6} html[data-color-mode=dark] [data-colorid=snkcps401k]{color:#5999f8}[data-colorid=b06d81ardo]{color:#bf2600} html[data-color-mode=dark] [data-colorid=b06d81ardo]{color:#ff6640}[data-colorid=d66q7d4ir9]{color:#0747a6} html[data-color-mode=dark] [data-colorid=d66q7d4ir9]{color:#5999f8}[data-colorid=oolgk25780]{color:#bf2600} html[data-color-mode=dark] [data-colorid=oolgk25780]{color:#ff6640}[data-colorid=degk7rsk3w]{color:#0747a6} html[data-color-mode=dark] [data-colorid=degk7rsk3w]{color:#5999f8}[data-colorid=dcfvnrhars]{color:#bf2600} html[data-color-mode=dark] [data-colorid=dcfvnrhars]{color:#ff6640}[data-colorid=djarv3a5u6]{color:#bf2600} html[data-color-mode=dark] [data-colorid=djarv3a5u6]{color:#ff6640}[data-colorid=co579hu3is]{color:#bf2600} html[data-color-mode=dark] [data-colorid=co579hu3is]{color:#ff6640}[data-colorid=c40gilfko7]{color:#0747a6} html[data-color-mode=dark] [data-colorid=c40gilfko7]{color:#5999f8}[data-colorid=jzl6t5qr0n]{color:#bf2600} html[data-color-mode=dark] [data-colorid=jzl6t5qr0n]{color:#ff6640}[data-colorid=q8cc8mhnrn]{color:#0747a6} html[data-color-mode=dark] [data-colorid=q8cc8mhnrn]{color:#5999f8}[data-colorid=qespx8gcnm]{color:#0747a6} html[data-color-mode=dark] [data-colorid=qespx8gcnm]{color:#5999f8}</style>
<ul>
<li><a class="not-blank" href="#Introduction"><span data-colorid="d66q7d4ir9">Introduction</span></a></li>
<li><a class="not-blank" href="#Blueprint"><span data-colorid="zfjfnjmq5c">Blueprint</span></a></li>
<li><a class="not-blank" href="#InfosysResponsibleAItoolkitSafetymethods"><span data-colorid="q8cc8mhnrn">Infosys Responsible AI toolkit - Safety methods</span></a></li>
<li><a class="not-blank" href="#UnStructuredText"><span data-colorid="snkcps401k">Un-Structured Text</span></a></li>
<li><a class="not-blank" href="#Image">Image</a>
<ul>
<li><a class="not-blank" href="#StructuredData"><span data-colorid="qespx8gcnm">Structured Data</span></a></li>
<li><a class="not-blank" href="#Code"><span data-colorid="oiugc8knng">Code</span></a></li>
<li><a class="not-blank" href="#Video"><strong><span data-colorid="dtgpnriefd">Video</span></strong></a></li>
</ul></li>

</ul>
</div>
<h2 id="Introduction"><span data-colorid="d66q7d4ir9">Introduction</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<p><span data-colorid="c40gilfko7">1.1. What is Safety Responsible AI?</span></p>
<p>Safety Responsible AI is a valuable tool that can help to develop and deploy AI systems in a safe and responsible manner. Safety Responsible AI is the practice of developing and deploying AI systems in a way that minimizes the risk of harm. This includes ensuring that AI systems are reliable, secure, and resilient to attacks. Safety Responsible AI is important because AI systems are increasingly being used in critical applications, such as healthcare, transportation, and finance. If AI systems fail or are compromised, it can have serious consequences for people and society. Safety responsible AI is a subset of responsible AI that focuses on ensuring that AI systems are safe and do not cause harm.</p>
<p>Ensuring that AI systems provide robust performance and are safe to use while minimizing the negative impact, through multiple checks like Profanity, toxicity, etc.</p>
<p><span data-colorid="jd06lgibi7">1.2. <u> Why is Safety Responsible AI important?</u></span></p>
<p><strong><u> </u></strong></p>
<p>·       <strong>Increased trust and adoption of AI:</strong> As people become more aware of the potential risks of AI, they are more likely to trust and adopt AI systems that are developed and deployed in a responsible manner.</p>
<p>·       <strong>Reduced risk of harm:</strong> Safety responsible AI helps to reduce the risk of harm to individuals and society from AI systems. This includes reducing the risk of bias, discrimination, and unintended consequences.</p>
<p>·       <strong>Compliance with regulations:</strong> As governments and regulators around the world develop new regulations for AI, safety responsible AI can help organizations to comply with these regulations.</p>
<p>·       <strong>Competitive advantage:</strong> Organizations that embrace safety responsible AI can gain a competitive advantage by demonstrating their commitment to developing and deploying AI systems in a safe and ethical manner.</p>
<p><span data-colorid="s50qt7skot">1.3. <u>The Principles of Safety Responsible AI</u></span></p>
<p>Safety Responsible AI is a framework that helps organizations to develop and deploy AI systems in a safe and responsible manner. The framework is based on the following principles:</p>
<p>·       <strong>Transparency:</strong> AI systems should be transparent and accountable, so that users can understand how they work and how they make decisions.</p>
<p>·       <strong>Fairness:</strong> AI systems should be fair and unbiased, so that they do not discriminate against any group of people.</p>
<p>·       <strong>Reliability:</strong> AI systems should be reliable and trustworthy, so that users can be confident that they will perform as expected.</p>
<p>·       <strong>Security:</strong> AI systems should be secure and protected from attack, so that they cannot be used to harm individuals or society.</p>
<p><span data-colorid="degk7rsk3w">1.4. <u>How Safety Responsible AI works?</u></span></p>
<p>AI systems provide robust performance and are safe to use while minimizing the negative impact, through multiple checks like Profanity, toxicity. Safety Responsible AI uses a four-step process:</p>
<ol start="1">
<li><p><strong>Risk assessment:</strong> The first step is to conduct a risk assessment to identify and assess the risks posed by the AI system. This includes identifying the potential for the system to cause harm to individuals or society, as well as the potential for the system to be used for malicious purposes.</p></li>
<li><p><strong>Design:</strong> Once the risks have been identified, the next step is to design the AI system in a way that mitigates these risks. This may involve using techniques such as transparency, fairness, reliability, and security.</p></li>
<li><p><strong>Testing:</strong> Once the AI system has been designed, it is important to test the system to ensure that it meets the safety and responsibility criteria. This includes testing the system for bias, accuracy, and robustness to attack.</p></li>
<li><p><strong>Deployment:</strong> Once the AI system has been tested and found to be safe and responsible, it can be deployed into production. However, it is important to continue to monitor the system for any potential problems.</p></li>
</ol>
<p><span data-colorid="s35x9512bd">1.4.1.     <u>Profanity</u></span></p>
<p>Profanity includes filtering out and preventing the use of profanity or harmful content. Profanity have mechanisms in place to detect and avoid generating or promoting offensive language. Profanity is a dictionary-based process that is the dictionary has the sets of offensive words. Profanity can be used as a tool to improve AI safety in several ways. Some of the profanities are:</p>
<p>·       <strong>To prevent AI systems from generating harmful or offensive content</strong>: Profanity can be used to identify and block profane words and phrases from being generated by AI systems. This can help to prevent AI systems from generating content that is hateful, discriminatory, or otherwise harmful.</p>
<p>·       <strong>To protect AI systems from adversarial attacks:</strong> Profanity can be used to identify and block adversarial attacks that attempt to exploit vulnerabilities in AI systems. For example, an attacker might try to input a profane word or phrase into an AI system to trick it into making a mistake. A profanity could be used to block this input, thereby protecting the AI system from attack.</p>
<p>·       <strong>To improve the transparency of AI systems:</strong> Profanity can be used to make AI systems more transparent and accountable. For example, a profanity could be used to highlight profane words and phrases in the output of an AI system. This could help users to understand how the AI system is making decisions and to identify any potential biases or errors in the system's output.</p>
<p> </p>
<p><span data-colorid="mxbnkxkiqw">1.4.2.     <u>Toxicity</u></span></p>
<p>Toxicity is a type of AI safety measure that can be used to detect and prevent toxic behavior from AI systems. Toxic behavior can include generating harmful or offensive content, amplifying harmful or offensive content, spreading misinformation, and manipulating users. Toxicity work by using LLM to identify patterns of toxic behavior. For example, a toxicity might be trained with known examples of toxic content, such as hate speech and violent threats. This would then be able to identify new examples of toxic content based on the patterns that it has learned. Toxicity can be used to improve AI safety in several ways. Some of the toxicities are:</p>
<p>·       <strong>To prevent AI systems from generating harmful or offensive content: </strong>Toxicity can be used to identify and block harmful or offensive content before it is generated by AI systems. This can help to prevent AI systems from generating content that is hateful, discriminatory, or otherwise harmful.</p>
<p>·       <strong>To protect AI systems from adversarial attacks:</strong> Toxicity can be used to protect AI systems from adversarial attacks that attempt to exploit vulnerabilities in the systems' natural language processing capabilities. For example, an attacker might try to input a toxic phrase into an AI system to trick it into generating toxic content. A toxicity could be used to block this input, thereby protecting the AI system from attack.</p>
<p>·       <strong>To improve the transparency of AI systems:</strong> Toxicity can be used to make AI systems more transparent and accountable. For example, a toxicity could be used to highlight potentially toxic content in the output of an AI system. This could help users to understand how the AI system is making decisions and to identify any potential biases or errors in the system's output.</p>
<h2 id="Blueprint"><span data-colorid="zfjfnjmq5c">Blueprint</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="efdf9ace-d084-4b0f-99fd-b6a6baa1f2e4" data-macro-name="expand" id="expander-1289422911">
<div class="expand-control" id="expander-control-1289422911" onclick="expandContent('expander-content-1289422911', 'expander-control-1289422911')">
<span class="expand-control-icon icon"> </span><span class="expand-control-text">Safety Blueprint</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1289422911">
<p> </p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="549" style="max-width: 760px;" width="539"><img alt="image (11)-20241114-081133.png" class="confluence-embedded-image image-center cursor-pointer" data-height="281" data-linked-resource-container-id="232359155" data-linked-resource-container-version="23" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image (11)-20241114-081133.png" data-linked-resource-id="1152188468" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="6dc4a0b8-5c21-46e6-a05a-00c5286d1a20" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/232359155/media/Qzl0UllxMTBXV3Vra3I2Zm5pMU02SWlFWUI0TG9LSUYtQkNpNEg3bF9jND0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVGswTmpRME5URTROQT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5Nak15TXpVNU1UVTEuYVcxaFoyVWxNakFvTVRFcExUSXdNalF4TVRFMExUQTRNVEV6TXk1d2JtYz0uZG1WeWMybHZiajB4Sm0xdlpHbG1hV05oZEdsdmJrUmhkR1U5TVRjek1UVTNNVGt4TURZek5pWmpZV05vWlZabGNuTnBiMjQ5TVNaaGNHazlkakltZDJsa2RHZzlOVE01Sm1obGFXZG9kRDB5TnpZPQ==/image__11_-20241114-081133.png" data-unresolved-comment-count="0" data-width="549" loading="lazy" name="image-attachment" src="/github-pages/images\image__11_-20241114-081133.png" style="width: 539px;" width="539"/></span>
<p> </p>
</div>
</div>
<p> </p>
<h2 id="InfosysResponsibleAItoolkitSafetymethods"><span data-colorid="q8cc8mhnrn">Infosys Responsible AI toolkit - Safety methods</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<h2 id="UnStructuredText"><span data-colorid="snkcps401k">Un-Structured Text</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="table-wrap dt391207007" style="max-width: 760px; width: 100%; margin: 10px 0px 0px;">
<div style="overflow-x: auto;"><table border="1" class="confluenceTable" data-layout="default" data-local-id="63667ab0-2720-4dcf-97ff-862da31995f8" data-table-width="760" style="border-collapse: collapse; width: 100%;">
<colgroup>
<col style="width: 70.0px;"/>
<col style="width: 107.0px;"/>
<col style="width: 202.0px;"/>
<col style="width: 187.0px;"/>
<col style="width: 194.0px;"/>
</colgroup>
<tbody>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong> </strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Training</strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Inferencing</strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Application Development</strong></p></td>
</tr>
<tr>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Traditional ML</strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Open-Source Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="u6g2xw07y8">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="djarv3a5u6">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="x6uag2cf20">To be identified</span></em></p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Custom built Profanity Filter /Model</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> <strong>Profanity Filter</strong></p><p><strong>(Toxic-Bert)</strong></p><p><strong>Model Life Cycle Stage: </strong>Data Preparation, Model Building and Training</p><p><strong>How:</strong></p>
<ul>
<li><p>Analyzing and filtering data for training with safety filters<br/>API's:<br/>Profanity/analyze/text</p></li>
<li><p>Analyses <code>toxicity</code></p></li>
<li><p><code>severe_toxicity</code></p></li>
<li><p><code>obscene</code></p></li>
<li><p><code>threat</code></p></li>
<li><p><code>insult</code></p></li>
<li><p><code>identity_attack</code></p></li>
<li><p><code>sexual_explicit</code></p></li>
</ul></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>NA</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>NA</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Commercial Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> AI Guardian</p><p><strong>Model Life Cycle Stage: </strong>To be identified</p><p><strong>How: </strong>To be identified</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> AI Guardian</p><p><strong>Model Life Cycle Stage: </strong>To be identified</p><p><strong>How: </strong>To be identified</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> AI Guardian</p><p><strong>Model Life Cycle Stage: </strong>To be identified</p><p><strong>How: </strong>To be identified</p></td>
</tr>
<tr>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Foundation Models </strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Open-Source Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="hke566y4t5">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="hjyc4at717">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="dcfvnrhars">To be identified</span></em></p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Custom built Profanity Filter</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>NA</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> <strong>Profanity Filter (Toxic-Bert)</strong></p><p><strong>Model Life Cycle Stage: </strong>Model Compliance and Approval (Test Prediction)</p><p><strong>How:</strong></p>
<ul>
<li><p>Filtering input text and output generated from LLM <br/>API's:<br/>Profanity/Analyze/text<br/>Profanity/Censor/text</p></li>
</ul></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> <strong>Profanity Filter (Toxic-Bert)</strong></p><p><strong>Model Life Cycle Stage: </strong>Production (Output Generation) and Monitoring</p><p><strong>How:</strong></p>
<ul>
<li><p>Filtering input text and output generated from LLM <br/>API's:<br/>Profanity/Analyze/text<br/>Profanity/Censor/text</p></li>
</ul></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Commercial Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="gauugy0jy5">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="y83nacmbgx">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="eqjspvkaft">To be identified</span></em></p></td>
</tr>
</tbody>
</table></div>
<style>html .dt391207007 table {min-width:532px;}</style>
</div>
<h2 id="Image">Image<a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="table-wrap dt176614871" style="max-width: 760px; width: 100%; margin: 10px 0px 0px;">
<div style="overflow-x: auto;"><table border="1" class="confluenceTable" data-layout="default" data-local-id="9db38af6-f8d3-4627-afc8-b52b7557dfb3" data-table-width="760" style="border-collapse: collapse; width: 100%;">
<colgroup>
<col style="width: 70.0px;"/>
<col style="width: 107.0px;"/>
<col style="width: 202.0px;"/>
<col style="width: 187.0px;"/>
<col style="width: 194.0px;"/>
<col style="width: 194.0px;"/>
<col style="width: 140.0px;"/>
</colgroup>
<tbody>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong> </strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Training</strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Inferencing</strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Application Development</strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
</tr>
<tr>
<td class="confluenceTd" rowspan="2" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Traditional ML</strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Open-Source Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> NSFWJS </p><p><strong>Model Life Cycle Stage: </strong>Model Building and Training</p><p><strong>How: </strong>It categorizes image probabilities in the following 5 classes:</p><p>· <strong>Drawing</strong> - safe for work drawings</p><p>· <strong>Hentai </strong>- hentai and pornographic drawings</p><p>· <strong>Neutral </strong>- safe for work neutral images</p><p>· <strong>Porn </strong>- pornographic images, sexual acts</p><p>· <strong>Sexy</strong> - sexually explicit images, not pornography</p><p>Blocks or blurs images with inappropriate content.</p><p><strong>API's:</strong><br/>safety/profanity/imageanalyze</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="co579hu3is">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="my0uzhhu4z">To be identified</span></em></p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Commercial Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> <strong>Nudity Detection API</strong> (yet to be explored)</p><p><strong>Model Life Cycle Stage: </strong>Data Preparation, Model Building and Training</p><p><strong>How:</strong></p>
<ul>
<li><p>Analyzing and filtering image data for nudity during training with safety filters<br/>API's:<br/>Profanity/analyze/image</p></li>
</ul></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="fkcqoxucyg">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="c6ojjoyzhx">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
</tr>
<tr>
<td class="confluenceTd" rowspan="2" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Foundation Models </strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Open-Source Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="fgbtlxmepz">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="eey1kuef49">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="jzl6t5qr0n">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Commercial Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>NA</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> Nudity Detection API (yet to be explored)</p><p><strong>Model Life Cycle Stage: </strong>Model Compliance and Approval (Test Prediction)</p><p><strong>How:</strong></p>
<ul>
<li><p>Analyzing and filtering LLM generated image for nudity using safety filters<br/>API's:<br/>Profanity/analyze/Image</p></li>
</ul></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> Nudity Detection API (yet to be explored)</p><p><strong>Model Life Cycle Stage: </strong>Production (Output Generation) and Monitoring</p><p><strong>How:</strong></p>
<ul>
<li><p>Analyzing and filtering LLM generated image for nudity using safety filters<br/>API's:<br/>Profanity/analyze/Image</p></li>
</ul></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
</tr>
</tbody>
</table></div>
<style>html .dt176614871 table {min-width:532px;}</style>
</div>
<h3 id="StructuredData"><span data-colorid="qespx8gcnm">Structured Data</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Tool evaluation for Structured data is applicable but Tools need to be explored yet.</p>
<h3 id="Code"><span data-colorid="oiugc8knng">Code</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<div class="table-wrap dt860203831" style="max-width: 760px; width: 100%; margin: 10px 0px 0px;">
<div style="overflow-x: auto;"><table border="1" class="confluenceTable" data-layout="default" data-local-id="629ebf8b-1830-431e-a1ff-64f9f5f43f56" data-table-width="760" style="border-collapse: collapse; width: 100%;">
<colgroup>
<col style="width: 70.0px;"/>
<col style="width: 107.0px;"/>
<col style="width: 202.0px;"/>
<col style="width: 187.0px;"/>
<col style="width: 194.0px;"/>
</colgroup>
<tbody>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong> </strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Training</strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Inferencing</strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Application Development</strong></p></td>
</tr>
<tr>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Traditional ML</strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Open-Source Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="twsj7se2is">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="m18kjpgwyn">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="khflsum4e7">To be identified</span></em></p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Custom built Profanity Filter</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>NA</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>NA</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>NA</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Commercial Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="vedwclvhsp">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="woczv200fn">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="kqhb8o6ro2">To be identified</span></em></p></td>
</tr>
<tr>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Foundation Models </strong></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Open-Source Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="xu7erfir9f">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="rcjmet9kuv">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="b06d81ardo">To be identified</span></em></p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Custom built Profanity Filter</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>NA</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> Needs to be custom built</p><p><strong>Model Life Cycle Stage: </strong>Model Compliance and Approval (Test Prediction)</p><p><strong>How:</strong></p>
<ul>
<li><p>Analyzing and filtering LLM generated code for Profanity using safety filters<br/>API's:<br/>Profanity/analyze/code</p></li>
</ul></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><strong>Tool:</strong> Needs to be custom built</p><p><strong>Model Life Cycle Stage: </strong>Production (Output Generation) and Monitoring</p><p><strong>How:</strong></p>
<ul>
<li><p>Analyzing and filtering LLM generated code for Profanity using safety filters<br/>API's:<br/>Profanity/analyze/code</p></li>
</ul></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Commercial Tools</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="oolgk25780">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="ea0pqlcxfw">To be identified</span></em></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><em><span data-colorid="jsairb90ih">To be identified</span></em></p></td>
</tr>
</tbody>
</table></div>
<style>html .dt860203831 table {min-width:532px;}</style>
</div>
<h3 id="Video"><strong><span data-colorid="dtgpnriefd">Video</span></strong><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Nudity Video</p>
<p><strong>Model</strong> - NudeNet package</p>
<p><strong>Process</strong> </p>
<ul>
<li><p>Converting Video into Frames and nudity content using the nudenet package</p></li>
<li><p>Concatenating all the frames and recreating the blurred nudity parts of the video.</p></li>
</ul>
<p><strong>API:</strong> safety/profanity/videosafety</p>
<p>NSFW Video</p>
<p><strong>Model</strong> - NSFW MODEL</p>
<p>Just like the image nsfw similarly works on the video Dividing the video into 5 categories as, ·</p>
<p>. <strong>Drawing</strong> - safe for work drawings</p>
<p>· <strong>Hentai </strong>- hentai and pornographic drawings</p>
<p>· <strong>Neutral </strong>- safe for work neutral images</p>
<p>· <strong>Porn </strong>- pornographic images, sexual acts</p>
<p>· <strong>Sexy</strong> - sexually explicit images, not pornography</p>
<p>Based on the scores the video will be blurred.</p>
<p><strong>Process</strong> </p>
<ul>
<li><p>Converting Video into Frames and nsfw model and based on the scores the frames will be blurred.</p></li>
<li><p>Concatenating all the frames and recreating the blurred NSFW parts of the video.</p></li>
</ul>
<p><strong>API:</strong> safety/profanity/nudvideosafetyAPI</p>


<p> </p>
<p> </p>
<p> </p>
</div>
<!-- ATTACHMENTS -->

<script>
            document.addEventListener("DOMContentLoaded", () => {
                const wrapper = document.getElementById("attachments-wrapper");
                const button = document.getElementById("toggle-attachments-view-button");
                document.querySelectorAll(".file-full").forEach(el => {
                    el.addEventListener("mouseover", moveTooltip);
                });

                button.addEventListener("click", () => {
                    wrapper.classList.toggle("attachments-wrapper-gallery");
                    wrapper.classList.toggle("attachments-wrapper-list");
                });
            });

            function moveTooltip(e) {
                if (e.target.classList.contains("file-wrapper")) {
                    let docWidth = document.body.clientWidth;
                    let docHeight = document.body.clientHeight;
                    let rect = e.target.getBoundingClientRect();
                    let fileTooltip = e.target.parentElement.querySelector(".file-tooltip")
                    if (fileTooltip) {
                        if (rect.left <= docWidth / 2) {
                            fileTooltip.classList.add("left");
                            fileTooltip.classList.remove("right");
                        } else {
                            fileTooltip.classList.remove("left");
                            fileTooltip.classList.add("right");
                        }
                        if (rect.top <= docHeight / 2) {
                            fileTooltip.classList.add("top");
                            fileTooltip.classList.remove("bottom");
                        } else {
                            fileTooltip.classList.remove("top");
                            fileTooltip.classList.add("bottom");
                        }
                    }
                }
            }

        </script>
<script>
                hideGroup('attachments');
            </script>
<div id="footer-comments-outlet">
<div>
<div class="page-comment-wrapper" data-testid="page-comment-wrapper">
<div class="cc-q82yp6">
<div class="_1e0c1txw _i0dl1osq _otyru2gc">
<div class="_bfhklbf8 _1bsbzwfg _4t3izwfg _2rko1ssb _19pk1b66 _2hwxutpp"></div>
<div class="_1e0c11p5 _yv0ehpgh _727q19bv _bfhk1j28 _1bsbdgin _18u0u2gc">
<div class="_nd5lzmlf _bfhklbf8 _y3gn1h6o _1yt45uws _19itglyw _2rko1l7b"></div>
<div class="_nd5lbahz _bfhklbf8 _y3gn1h6o _1yt41h4g _19itglyw _2rko1l7b"></div>
</div>
</div>
<div class="_1sb2f705 _1e0c1ule _otyrpxbi _ca0qutpp _n7zl1l7n _1bsb1osq"></div>
<div class="_1e0c1txw _1n261g80" data-testid="comment-container">
<div class="_1e0c1txw _i0dl1osq _otyru2gc">
<div class="_bfhklbf8 _1bsbzwfg _4t3izwfg _2rko1ssb _19pk1b66 _2hwxutpp"></div>
<div class="_1bsb1osq _19itglyw _2rko1l7b _4t3i1ylp _syaz9s69 _ca0qze3t _1e0c1o8l _s7n4jp4b _18u0u2gc _16jlkb7n _bfhklbf8"></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="inline-comments-outlet"></div>
</div>
</div></body><br/><br/></html>