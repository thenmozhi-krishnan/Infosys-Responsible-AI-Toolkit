<!DOCTYPE html>
<html><head></head><body><div class="main-panel wiki-content">
<div class="description">
<div class="header-wrapper" style="max-width: initial">
<!-- breadcrumbs could be implemented and inserted here -->
<div></div>

</div>
<div class="header-wrapper">
<div class="title-wrapper">
<div class="title">
<h1 style="display: flex; align-items: center;">
                                        Technical Details
                                    </h1>
</div>
<div class="row" style="display: flex;padding: 0; ">

</div>
</div>
</div>
<div class="content-wrapper">
<style>@media (prefers-color-scheme: dark) { }</style>
<div class="toc-macro client-side-toc-macro conf-macro output-block" data-cssliststyle="none" data-hasbody="false" data-headerelements="H1,H2" data-layout="default" data-local-id="30653e4c-4651-4783-b01c-0407091fa4f1" data-macro-id="61b13346-6f4e-46c3-bb1b-e104763c8304" data-macro-name="toc" data-numberedoutline="false" data-structure="list">
<style>[data-colorid=edcmnu42l7]{color:#ff991f} html[data-color-mode=dark] [data-colorid=edcmnu42l7]{color:#e07a00}</style>
<ul>
<li><a class="not-blank" href="#HandlingHallucinationsinLLMs">Handling Hallucinations in LLMs</a></li>
<li><a class="not-blank" href="#HallucinationMetrics">Hallucination Metrics</a></li>
<li><a class="not-blank" href="#CostCalculation">Cost Calculation</a></li>
<li><a class="not-blank" href="#MultimodalRAG">Multi-modal RAG</a></li>
<li><a class="not-blank" href="#RAGChatbot"><strong>RAG Chatbot</strong></a></li>
</ul>
</div>
<h2 id="HandlingHallucinationsinLLMs">Handling Hallucinations in LLMs<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<ul>
<li><p>Retrieval Augment Generation pattern can be used to ground the LLM response within the context provided.​</p></li>
<li><p>The context can be retrieved by searching in the organization’s knowledge-based or document repository.​</p></li>
<li><p>Moreover, citations to the referred documents or articles can also be provided in the final response</p></li>
</ul><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="677" style="max-width: 760px;" width="710"><img class="confluence-embedded-image image-center cursor-pointer" data-height="343" data-linked-resource-container-id="972881923" data-linked-resource-container-version="12" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20230622-134009.png" data-linked-resource-id="985268985" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="a1955663-5a75-4c0f-8ee0-b3f3bba9b6a2" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/308019381/972881923/media/aDBSUERRQjBfSUs1V1pEVGhoWUU1S2FJM0NqRGtINVZKQ011SU1GVFE2Yz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVEkzTVRRek9EYzJNUT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5PVGN5T0RneE9USXouYVcxaFoyVXRNakF5TXpBMk1qSXRNVE0wTURBNUxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3lOVGswTVRJMk1qY3dOaVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TnpFd0ptaGxhV2RvZEQwek5Uaz0=/image-20230622-134009.png" data-unresolved-comment-count="0" data-width="677" loading="lazy" name="image-attachment" src="/docs/images\image-20230622-134009.png" style="width: 710px;" width="710"/><span class="image-caption"><p>Handling Hallucinations</p></span></span>
<p> </p>
<h2 id="HallucinationMetrics">Hallucination Metrics<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<h3 id="ContextualHallucination">Contextual Hallucination<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Contextual hallucination highlights the limitations of LLMs in truly understanding context and generating responses that reflect genuine comprehension or knowledge.</p>
<h3 id="HallucinationScoreInfosys">Hallucination Score (Infosys)<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>The hallucination score is determined through G- Eval metrics values and assessing similarity score across multiple categories, including input prompt to output prompt, input prompt to source, and output prompt to source. This score is a weighted average that considers the scores from these below categories.</p>
<p>we scaled the avg score of G-eval</p>
<p>avgmetrics = ((<code>faithfullness[]+Relevance[]+ Adherence[2]+ correctness [])/4/5</code></p>
<h5 id="WhentheaveragescoreofGEvalMetricsavgmetricsisgreaterthan075" style="margin-left: 30.0px;">When the average score of G- Eval Metrics (avgmetrics) is greater than 0.75<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h5>
<p style="padding-left: 60px;">Hallucination score = 1 - avgmetrics</p>
<h5 id="WhentheaveragescoreofGEvalMetricsisbetween05and075" style="margin-left: 30.0px;">When the average score of G- Eval Metrics is between <strong>0.5 </strong>and<strong> 0.75</strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h5>
<p style="padding-left: 60px;">we calculate the average prompt similarity score and classify them based on the maximum prompt similarity score</p>
<p style="padding-left: 60px;"><code>avgsimilarity=(inpoutsim+ressourcescore+inpsourcesim)/3</code></p>
<p style="padding-left: 60px;"><code>maxscore</code> = max(<code>inpoutsim+ressourcescore+inpsourcesim)</code></p>
<p style="padding-left: 60px;"><code>if maxscore&gt;0.75</code></p>
<p style="padding-left: 60px;">Hallucination score = <code>1-avgsimilarity*0.8-avgmetrics*0.2</code></p>
<p style="padding-left: 60px;"><code>if maxscore&gt;=0.5 and maxscore&lt;0.75</code></p>
<p style="padding-left: 60px;">Hallucination score = <code>1-avgsimilarity*0.2-avgmetrics*0.8</code></p>
<p style="padding-left: 60px;"><code>if maxscore&lt;0.5</code></p>
<p style="padding-left: 60px;">Hallucination score = <code>1-avgsimilarity*0.5-avgmetrics*0.5</code></p>
<h5 id="WhenaveragescoreofGEvalMetricsislessthan05">·       When average score of G- Eval Metrics<strong> is less than 0.5</strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h5>
<p style="padding-left: 60px;">Hallucination Score = 1 - avgmetrics</p>
<p style="padding-left: 30px;">Here, <code>inpoutsim</code> represents the Input-Output similarity score, <code>inpsourcesim</code> represents the Input-Source similarity score, and <code>ressourcescore</code> represents the Output-Source similarity score.</p>
<h3 id="ForNonRAGScenario">For Non - RAG Scenario<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>we are using the prompting technique to get the Hallucination score based on the Relevance, Factual Accuracy. The range of the score is between 0 to 1.</p>
<ul>
<li><p><code>0 indicates the answer is highly relevant to the prompt, it is highly realistic, the answer contains no factual errors and the answer is not at all nonsensical.</code></p></li>
<li><p><code>1 indicates the answer is highly unrelated to the prompt, it is highly implausible or unrealistic, it is completely factually incorrect and highly nonsensical.</code></p></li>
</ul>
<p>To interpret the score, we had asked to avoid assigning the score 0.5</p>
<h3 id="PerplexityScore"><strong>Perplexity Score</strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Perplexity is a measure on how well the model predicts the next word or character based on the context provided by the previous words or characters. The <strong>lower</strong> the perplexity score, the better the model’s ability to predict the next word accurately.</p>
<p>For example, if a language model predicts that there is a 0.5 chance that the next word is "dog" and a 0.5 chance that it is "cat", the probability distribution would be [0.5, 0.5]. The geometric mean of these probabilities would be the square root of their product, which in this case is 0.7071. The perplexity score would then be the inverse of this value, or approximately 1.4142. This means that the model would be slightly surprised to see either "dog" or "cat" as the next word given the context provided by the previous words or characters. If the model were perfect and predicted the correct word with certainty, its perplexity score would be 1. If it performed poorly and predicted each possible output equally likely, its perplexity score would be infinity.</p>
<p>Perplexity is calculated using average cross-entropy, which in turn is calculated using the number of words in the data set and the predicted probability of a word (target word) as per the preceding context. </p>
<p>Where N is the total number of words in the data set and P(w_i | w_1, w_2, …, w_{i-1}) is the predicted probability of the i-th word given the preceding context (w_1, w_2, …, w_{i-1})</p>
<p>Source:<a class="external-link" data-card-appearance="inline" href="https://ramblersm.medium.com/the-significance-of-perplexity-in-evaluating-llms-and-generative-ai-62e290e791bc%20" rel="nofollow" target="_blank">https://ramblersm.medium.com/the-significance-of-perplexity-in-evaluating-llms-and-generative-ai-62e290e791bc </a></p>
<h3 id="Coherence">Coherence<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>The collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby "the summary should be well-structured and well-organized. The summary should not just be a heap of related information but should build from sentence to a coherent body of information about a topic." GPT model is used as an evaluator, and this is a prompt-based evaluation method.</p>
<h3 id="Consistency">Consistency<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>The factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. GPT model is used as an evaluator, and this is a prompt-based evaluation method. </p>
<h3 id="Relevance">Relevance<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Selection of important content from the source. The summary should include only important information from the source document. Annotators were instructed to penalize summaries which contained redundancies and excess information. GPT model is used as an evaluator, and this is a prompt-based evaluation method.</p>
<h3 id="Fluency">Fluency<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>The quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure. GPT model is used as an evaluator, and this is a prompt-based evaluation method.</p>
<p>Source: <a class="external-link" href="https://arxiv.org/pdf/2303.16634.pdf" rel="nofollow" target="_blank">https://arxiv.org/pdf/2303.16634.pdf</a></p>
<h3 id="Uncertainty">Uncertainty<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Uncertainty refers to the LLM's lack of confidence in the correctness or accuracy of its generated text. When an LLM is presented with a prompt, it can generate multiple possible continuations or completions. The uncertainty associated with each completion reflects the LLM's internal assessment of how likely each continuation is to be the most appropriate one.</p>
<p style="padding-left: 30px;"><strong>Structural Uncertainty</strong>: We use normalized entropy to calculate how uncertain the model was in each token selection. If the model leans heavily towards one answer, the entropy is low. But if it's torn between multiple options, entropy spikes. The normalization step ensures we're comparing things consistently across different prompts.</p>
<p style="padding-left: 30px;"><strong>Conceptual Uncertainty:</strong> For each sampled token in the response, we create a 'partial' version of the potential response up to that token. Each of these partial responses is transformed into an embedding. We then measure the distance between this partial response and the model's final, complete response. This tells us how the model's thinking evolves as it builds up its answer.</p>
<p style="padding-left: 30px;">Where MCD is the mean cosine distance between all choice embeddings, C is the number of choices, D represents the set of average cosine distances calculated for each partial response relative to its complete response.</p>
<p style="padding-left: 30px;">If Structural Uncertainty is low but Conceptual Uncertainty is high, the model is clear about the tokens it selects but varies significantly in the overall messages it generates. This could imply that the model understands the syntax well but struggles with maintaining a consistent message.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="222" style="max-width: 760px;" width="250"><img alt="image-20240416-095709.png" class="confluence-embedded-image image-center cursor-pointer" data-height="82" data-linked-resource-container-id="972881923" data-linked-resource-container-version="12" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240416-095709.png" data-linked-resource-id="984908136" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="77fa759c-845c-495c-abce-a9299d599adf" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/308019381/972881923/media/bjJfQ1RZU2RtZy10bjJqNGtOSEE1VU81Z2lUclJoSlFNWTROMDJxNHlnaz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVEkzTVRRek9EYzJNZz09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5PVGN5T0RneE9USXouYVcxaFoyVXRNakF5TkRBME1UWXRNRGsxTnpBNUxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3lOVGswTVRNek5qYzVOaVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TWpVd0ptaGxhV2RvZEQwNU1RPT0=/image-20240416-095709.png" data-unresolved-comment-count="0" data-width="222" loading="lazy" name="image-attachment" src="/docs/images\image-20240416-095709.png" style="width: 250px;" width="250"/></span>
<p> </p>
<p style="padding-left: 30px;">H <sub>normalized </sub>is normalized entropy.</p>
<p style="padding-left: 30px;">Conversely, high Structural Uncertainty and low Conceptual Uncertainty could indicate that the model is unsure at the token-level but consistent in the overall message. Here, the model knows what it wants to say but struggles with how to say it precisely.</p>
<p style="padding-left: 30px;">If both are high or both are low, it may suggest that the token-level uncertainty and overall message uncertainty are strongly correlated for the specific task, either both being well-defined or both lacking clarities.</p>
<p style="padding-left: 30px;">Source: <a class="external-link" data-card-appearance="inline" href="https://www.watchful.io/blog/decoding-llm-uncertainties-for-better-predictability" rel="nofollow" target="_blank">https://www.watchful.io/blog/decoding-llm-uncertainties-for-better-predictability</a></p>
<h3 id="TokenImportance">Token Importance<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>To calculates the importance of individual tokens in a text prompt by comparing the original embedding with an embedding where a single token is removed.</p>
<h4 id="Methods" style="margin-left: 30.0px;">Methods<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p style="padding-left: 30px;">Tokenization: Break the prompt down into its individual words or 'tokens.'</p>
<p style="padding-left: 30px;">Text Embedding: Convert the text prompt into a numerical format, known as an 'embedding.'</p>
<p style="padding-left: 30px;">Ablation and Re-Embedding: Remove each token one by one, create a new embedding, and then compare it to the original.</p>
<p style="padding-left: 30px;">Importance Estimation: The degree of difference gives us a rough idea of each token's importance.</p>
<h4 id="WhenItMayBeUseful" style="margin-left: 30.0px;">When It May Be Useful?<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p style="padding-left: 30px;">High-Quality Embeddings: If the embeddings are robust, the token importance estimates tend to be more reliable.</p>
<p style="padding-left: 30px;">Simple, Unambiguous Prompts: Clear and straightforward prompts are where this method seems to offer the most reliable results.</p>
<h4 id="Limitations" style="margin-left: 30.0px;">Limitations<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p style="padding-left: 30px;">Complex Prompts: Ambiguity or complexity in the prompt can lead to less reliable estimates.</p>
<p style="padding-left: 30px;">Typical Prompt Lengths: e.g. very short or very long prompts. These results tend to still be correlated but deviate a bit more than prompts of “average” length.</p>
<h4 id="ModelsUsed" style="margin-left: 30.0px;">Models Used<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p style="padding-left: 30px;">GPT-2 embeddings</p>
<p style="padding-left: 30px;">GPT-3 (ada-002) embeddings</p>
<p style="padding-left: 30px;">GPT-3.5 (ada-002) embeddings</p>
<p style="padding-left: 30px;"><strong><span data-colorid="edcmnu42l7">GPT-4 not compatible</span></strong></p>
<p style="padding-left: 30px;">Source:<strong> </strong><a class="external-link" data-card-appearance="inline" href="https://www.watchful.io/blog/a-surprisingly-effective-way-to-estimate-token-importance-in-llm-prompts" rel="nofollow" target="_blank">https://www.watchful.io/blog/a-surprisingly-effective-way-to-estimate-token-importance-in-llm-prompts</a></p>
<h2 id="CostCalculation">Cost Calculation<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<p>Based on the model chosen, the cost is primarily based on the number of tokens in the prompt and the response. We have implemented the subsequent mathematical expression to ascertain the financial implications for the Chain of Thought (CoT) and Thread of Thought (ThoT).</p>
<p><code>total_cost = ((input_tokens / 1000) * prompt_price_per_1000_tokens) + ((output_tokens / 1000) * response_price_per_1000_tokens)</code></p>
<p>where the <code>input_tokens</code>is prompt entered and the <code>output_tokens</code> is generated response.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="1277" style="max-width: 760px;" width="750"><img alt="image-20240909-090832.png" class="confluence-embedded-image image-center cursor-pointer" data-height="577" data-linked-resource-container-id="972881923" data-linked-resource-container-version="12" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240909-090832.png" data-linked-resource-id="985825705" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="29b6a944-d60a-4434-89f9-c8e8df518a8b" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/308019381/972881923/media/cnVvZGdpdFhCckJJN1JoRDFHd3R3V2R3ZEZOSVlhUDlPUnZKRGlGc3NhTT0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVEkzTVRRek9EYzJNZz09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5PVGN5T0RneE9USXouYVcxaFoyVXRNakF5TkRBNU1Ea3RNRGt3T0RNeUxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3lOVGswTVRRd05qQXhOeVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TnpVd0ptaGxhV2RvZEQwek16az0=/image-20240909-090832.png" data-unresolved-comment-count="0" data-width="1277" loading="lazy" name="image-attachment" src="/docs/images\image-20240909-090832.png" style="width: 750px;" width="750"/></span>
<h2 id="MultimodalRAG">Multi-modal RAG<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<h3 id="ImageRetrievalRAG"><strong>Image Retrieval RAG</strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>We have implemented the feature by importing necessary modules such as <code>base64</code> for image encoding, <code>BytesIO</code> for handling byte streams, and <code>Image</code> from PIL for image processing.</p>
<p><strong>Steps</strong></p>
<p>The main class in code includes three methods:</p>
<ul>
<li><p><strong>encode_image(self, image):</strong> This method takes an image file as input, opens it, determines its format, saves it to a BytesIO buffer in the same format, and then encodes this buffer into a Base64 string. The Base64 string is then returned.</p></li>
<li><p><strong>config(self, messages, modelName):</strong> This method seems to interact with the AzureChatOpenAI class, possibly sending messages to an AI model deployed on Azure. The exact model used is determined by the <code>modelName</code> parameter. It then returns the AI-generated response.</p></li>
<li><p><strong>image_rag(self, payload):</strong> This is the main method of the class. It takes a payload containing text and file data. It uploads each file to Azure storage, encodes the image into a Base64 string, and adds this to the messages list alongside the text. It then sends these messages to the AI model via the <code>config</code> method and returns the response.</p></li>
</ul>
<h3 id="VideoRetrievalRAG"><strong>Video Retrieval RAG</strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Video RAG pipeline uses various libraries to extract and analyze content from a video file. It primarily focuses on converting video frames into base64-encoded images and extracting audio to generate a transcript. It utilizes libraries such as <span style="background-color: rgb(220,223,228);">cv2</span> for video processing, <span style="background-color: rgb(220,223,228);">moviepy</span> for audio extraction, and <span style="background-color: rgb(220,223,228);">speech_recognition</span> for converting audio to text.</p>
<p><strong>Steps</strong></p>
<p>The main class in code includes three methods:</p>
<p> <strong>video_rag(payload):</strong> This method:</p>
<ul>
<li><p>Receives a payload containing prompt and video file.</p></li>
<li><p>Processes the video to extract frames and audio using the process_video function.</p></li>
<li><p>Converts the audio to text using convert_audio_to_text.</p></li>
<li><p>Interacts with an Azure OpenAI model to generate a response based on the extracted frames and audio transcript.</p></li>
</ul>
<p><strong>process_video(video_path, seconds_per_frame):</strong> Handles video processing by:</p>
<ul>
<li><p>Extracting frames at specified intervals.</p></li>
<li><p>Extracting audio and saving it as a WAV file.</p></li>
</ul>
<p><strong>convert_audio_to_text(audio_path):</strong> Uses speech recognition to transcribe audio into text.</p>
<h2 id="RAGChatbot"><strong>RAG Chatbot</strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<p>We have implemented a chatbot using the <span style="background-color: rgb(220,223,228);">LangChain</span> library for a retrieval-based question-answering system. It initializes necessary modules, sets up logging, and configures the chatbot with a specific prompt template. </p>
<ul>
<li><p>The <code>vectorretriever_generation</code> function generates a retriever object from a list of <span style="background-color: rgb(220,223,228);">blob names</span>. It creates a unique directory to store the downloaded PDF files, retrieves the files from a specified URL, and loads them using the <code>DirectoryLoader</code> and <code>PyPDFLoader</code> classes. </p></li>
<li><p>The <span style="background-color: rgb(220,223,228);">qachatbot</span> function handles the question-answering process.</p></li>
<li><p>The <code>RetrievalQA</code> class is used to create a question-answering chain with the language model and retriever. The response includes both the answer to the user's query and a list of related questions which can be ask by user further in chatbot.</p></li>
</ul>
<p>The response is formatted as follows:</p>
<ul>
<li><p><strong>Answer</strong>: The main answer to the user's query.</p></li>
<li><p><strong>Related Questions</strong>: A list of related questions that can be explored further by user within chatbot.</p></li>
</ul>
</div>
<!-- ATTACHMENTS -->

<script>
            document.addEventListener("DOMContentLoaded", () => {
                const wrapper = document.getElementById("attachments-wrapper");
                const button = document.getElementById("toggle-attachments-view-button");
                document.querySelectorAll(".file-full").forEach(el => {
                    el.addEventListener("mouseover", moveTooltip);
                });

                button.addEventListener("click", () => {
                    wrapper.classList.toggle("attachments-wrapper-gallery");
                    wrapper.classList.toggle("attachments-wrapper-list");
                });
            });

            function moveTooltip(e) {
                if (e.target.classList.contains("file-wrapper")) {
                    let docWidth = document.body.clientWidth;
                    let docHeight = document.body.clientHeight;
                    let rect = e.target.getBoundingClientRect();
                    let fileTooltip = e.target.parentElement.querySelector(".file-tooltip")
                    if (fileTooltip) {
                        if (rect.left <= docWidth / 2) {
                            fileTooltip.classList.add("left");
                            fileTooltip.classList.remove("right");
                        } else {
                            fileTooltip.classList.remove("left");
                            fileTooltip.classList.add("right");
                        }
                        if (rect.top <= docHeight / 2) {
                            fileTooltip.classList.add("top");
                            fileTooltip.classList.remove("bottom");
                        } else {
                            fileTooltip.classList.remove("top");
                            fileTooltip.classList.add("bottom");
                        }
                    }
                }
            }

        </script>
<script>
                hideGroup('attachments');
            </script>
<div id="footer-comments-outlet">
<div>
<div class="page-comment-wrapper" data-testid="page-comment-wrapper">
<div class="cc-q82yp6">
<div class="_1e0c1txw _i0dl1osq _otyru2gc">
<div class="_bfhklbf8 _1bsbzwfg _4t3izwfg _2rko1ssb _19pk1b66 _2hwxutpp"></div>
<div class="_1e0c11p5 _yv0ehpgh _727q19bv _bfhk1j28 _1bsbdgin _18u0u2gc">
<div class="_nd5lzmlf _bfhklbf8 _y3gn1h6o _1yt45uws _19itglyw _2rko1l7b"></div>
<div class="_nd5lbahz _bfhklbf8 _y3gn1h6o _1yt41h4g _19itglyw _2rko1l7b"></div>
</div>
</div>
<div class="_1sb2f705 _1e0c1ule _otyrpxbi _ca0qutpp _n7zl1l7n _1bsb1osq"></div>
<div class="_1e0c1txw _1n261g80" data-testid="comment-container">
<div class="_1e0c1txw _i0dl1osq _otyru2gc">
<div class="_bfhklbf8 _1bsbzwfg _4t3izwfg _2rko1ssb _19pk1b66 _2hwxutpp"></div>
<div class="_1bsb1osq _19itglyw _2rko1l7b _4t3i1ylp _syaz9s69 _ca0qze3t _1e0c1o8l _s7n4jp4b _18u0u2gc _16jlkb7n _bfhklbf8"></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="inline-comments-outlet"></div>
</div>
</div></body><br/><br/></html>