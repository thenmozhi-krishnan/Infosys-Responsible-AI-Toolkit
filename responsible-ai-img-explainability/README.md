# Responsible-AI-Image-Explain

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Installation](#installation)
- [Set Configuration Variables](#set-configuration-variables)
- [Running the Application](#running-the-application)
- [License](#license)
- [Contact](#contact)

## Introduction

**Image Explain** offers detailed explanations for images generated by Large Language Models (LLMs). It provides an in-depth analysis of the image, highlighting key insights such as the presence of watermarks, detection of potential biases, and identification of the visual style used. In addition, the system evaluates the image using key metrics like creativity and certainty.

Image Explain also performs object detection, identifying and labeling objects within the image. It then provides an explanation based on the detected objects, This enhances the interpretability and trustworthiness of AI-generated visuals by combining both image-level and object-level understanding.


## Features
- **Watermark** 
    Identifies and assesses any embedded marks or logos within the image.
- **Bias** 
    Evaluates the image for potential biases that may reflect unequal or prejudiced representations.
- **Style**
    Classifies the image's style (e.g., Cartoon, Anime, Photorealism) by analyzing visual elements and distinctive characteristics.
- **Creativity Score**
    Quantitatively evaluates an image's visual appeal based on composition, color harmony, texture, and subject matter.
- **Relevance Score**
    Assesses how well the generated image aligns with the given prompt, indicating its accuracy in matching the intended description or context.

## Installation
To run the application, first we need to install Python and the necessary packages:

1. Install Python (version 3.11.x) from the [official website](https://www.python.org/downloads/) and ensure it is added to your system PATH.

2. Clone the repository:
    ```sh
    git clone <repository-url>
    ```

3. Navigate to the `responsible-ai-image-explain` directory:
    ```sh
    cd responsible-ai-image-explain
    ```

4. Create a virtual environment:
    ```sh
    python -m venv venv
    ```

5. Activate the virtual environment:
    - On Windows:
        ```sh
        .\venv\Scripts\activate
         ```
6. Upgrade pip:
    ```sh
    python -m pip install --upgrade pip
    ```

7. Go to the `requirements` directory where the `requirement.txt` file is present and install the requirements:
    ```sh
    pip install -r requirements.txt
    ```
8. ## CLIP ViT-B/16 – Vision-Language Model by OpenAI

    **CLIP-ViT-B/16** is a powerful vision-language model developed by OpenAI that aligns images and text in a shared embedding space. It supports tasks like **zero-shot classification**, **image-text retrieval**, and more.

    ### Model Architecture

    - **Image Encoder:** ViT-B/16 (Vision Transformer with 16×16 patches)
    - **Text Encoder:** Transformer-based
    - **Token Limit:** 77 tokens (text)
    - **Patch Size:** 16×16
    - **Layers:** 12
    - **Hidden Size:** 768
    - **Parameters:** ~149 million

    ---

    ## Downloading `openai/clip-vit-base-patch16` from Hugging Face

    This provides step-by-step instructions for downloading the model files for [`openai/clip-vit-base-patch16`](https://huggingface.co/openai/clip-vit-base-patch16), either manually or using command-line tools like `curl`.

    ---

    ## Model Repository

    - **Model Page:** https://huggingface.co/openai/clip-vit-base-patch16
    - **Files Directory:** https://huggingface.co/openai/clip-vit-base-patch16/tree/main

    ---

    ## Required Files

    To use this model locally, download the following files:

    - `config.json`
    - `pytorch_model.bin`
    - `preprocessor_config.json`
    - `merges.txt`
    - `vocab.json`
    - `tokenizer_config.json`
    - `special_tokens_map.json`
    - `tokenizer.json`

    ---

    ## Approach-1: Manual Download

    1. Go to: [https://huggingface.co/openai/clip-vit-base-patch16/tree/main](https://huggingface.co/openai/clip-vit-base-patch16/tree/main)
    2. For each of the required files listed above:
        - Click the **Download** button in the top-right corner
        - Save the file to your local machine
    3. Ensure all 8 required files are downloaded
    ---

    ## Approach-2: Download with `curl`

    ```bash
    # Download the files using curl
    curl -L -o config.json https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/config.json
    curl -L -o pytorch_model.bin https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/pytorch_model.bin
    curl -L -o preprocessor_config.json https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/preprocessor_config.json
    curl -L -o merges.txt https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/merges.txt
    curl -L -o vocab.json https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/vocab.json
    curl -L -o tokenizer_config.json https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/tokenizer_config.json
    curl -L -o special_tokens_map.json https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/special_tokens_map.json
    curl -L -o tokenizer.json https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/tokenizer.json
    ```
    After downloading all files:

    1. Create the **clip-vit-base-patch16** directory under **responsible-ai-img-explainability\responsible-ai-image-explain\model**
    2. Move all downloaded files to the clip-vit-base-patch16 directory

> **Note**: Verify that all 8 required files are successfully moved to the **responsible-ai-img-explainability\responsible-ai-image-explain\model\clip-vit-base-patch16** before proceeding.

## Set Configuration Variables

After installing all the required packages, configure the variables necessary to run the APIs.

1. Navigate to the `image_explain` directory:
    ```sh
    cd ..
    cd src/image_explain
    ```

2. Locate the `.env` file, which contains keys like the following:

    ```sh
    ALLOWED_ORIGINS = "${allowedorigins}"                    # [Mandatory]
    BIAS_DETECTION_API = "${bias_detection_api}"             # [Mandatory]
    AZURE_OPENAI_API_KEY = "${azure_openai_api_key}"         # [Mandatory]
    AZURE_OPENAI_ENDPOINT = "${azure_openai_endpoint}"       # [Mandatory]
    AZURE_OPENAI_API_VERSION = "${azure_openai_api_version}" # [Mandatory]
    AZURE_DEPLOYMENT_ENGINE = "${azure_deployment_engine}"   # [Mandatory]
    GEMINI_API_KEY = "${gemini_api_key}"                     # [Optional]
    GEMINI_MODELNAME = "${gemini_modelname}"                 # [Optional]
    ```

    **NOTE:**
   ```sh
     BIAS_DETECTION_API = "${bias_detection_api}"
     Get this API(/api/v1/fairness/analysis/image) from **responsible-ai-fairness** repository and Ensure that the fairness repository is running.
     If you provide above API, it will be used to perform bias detection.
     If no API is provided, the system will proceed without performing bias detection and will continue with other operations as usual.
    ```
    ```sh
    To allow access to all sites, use the value "*" in "${allowedorigins}". Alternatively, you can specify a list of sites that should have access.
    ```

4. Replace the placeholders with your actual values.

## Running the Application

Once we have completed all the aforementioned steps, we can start the service.

1. Navigate to the `src` directory:
    ```sh
    cd ..
    ```

2. Run `main.py` file:
    ```sh
    python main.py
    ```

3. Use the Port No that is mentioned in main.py file. Open the swagger URL in browser once server is running:

   `http://localhost:31016/rai/v1/image-explainability/docs`
    
For API calls, please refer to the [API Documnet](responsible-ai-image-explain/docs/API_Doc.pdf)

## License

The source code for the project is licensed under MIT license, which you can find in the [LICENSE.md](LICENSE.md) file.

## Contact

If you have more questions or need further insights, feel free to Connect with us @ infosysraitoolkit@infosys.com
